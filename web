>>>TRUENAS
-Open VM → select truenas osi→ select linux → 8gb ram / 2 cpu → install→ boot with bios→ after that ip –. Open website through ip→ root/root passwd→then create pool → according Q

You said:
Create a CI/CD pipeline using Jenkins, Docker, and GitHub to automate the web by using the NGINX image. with step by step instructions and use aws debian only and use 2 instance and also how to setup jenkins 



2/2

ChatGPT said:
Creating a CI/CD pipeline using Jenkins, Docker, and GitHub on AWS Debian-based EC2 instances involves multiple components. Below is a step-by-step guide to:

Set up Jenkins on one EC2 instance

Use another EC2 instance for deployment (Docker + NGINX)

Automate deployment of a static website hosted on GitHub

Use NGINX as the web server

Trigger deployment via GitHub commits

🔧 Architecture Overview
Instance 1 (Jenkins Server): Controls CI/CD pipeline

Instance 2 (Docker Host): Runs Docker + NGINX to serve your website

GitHub Repo: Contains your static website (HTML/CSS/JS)

🧱 Prerequisites
2 x AWS EC2 instances (Debian-based)

Open these ports:

Jenkins (8080) on Instance 1

HTTP (80) on Instance 2

Generate SSH key on Jenkins instance to allow it to SSH into the deploy server (Instance 2)

✅ Step 1: Launch Two AWS EC2 Debian Instances
Use Debian 12 or similar

Assign Elastic IPs for easier access

Tag them for easy management (Jenkins, DockerHost)

🚀 Step 2: Install Jenkins on Instance 1
SSH into Instance 1 (Jenkins)
bash
Copy
Edit
sudo apt update && sudo apt install -y openjdk-17-jdk curl gnupg2 git
Add Jenkins Repo and Install
bash
Copy
Edit
curl -fsSL https://pkg.jenkins.io/debian-stable/jenkins.io-2023.key | sudo tee \
  /usr/share/keyrings/jenkins-keyring.asc > /dev/null

echo deb [signed-by=/usr/share/keyrings/jenkins-keyring.asc] \
  https://pkg.jenkins.io/debian-stable binary/ | sudo tee \
  /etc/apt/sources.list.d/jenkins.list > /dev/null

sudo apt update
sudo apt install -y jenkins
Start Jenkins
bash
Copy
Edit
sudo systemctl enable jenkins
sudo systemctl start jenkins
Open Jenkins in browser
Visit: http://<JENKINS_PUBLIC_IP>:8080

Use the password from:

bash
Copy
Edit
sudo cat /var/lib/jenkins/secrets/initialAdminPassword
🐳 Step 3: Set Up Docker Host on Instance 2
SSH into Instance 2 (DockerHost)
bash
Copy
Edit
sudo apt update && sudo apt install -y docker.io nginx
sudo systemctl enable docker && sudo systemctl start docker
Test Docker:
bash
Copy
Edit
sudo docker run hello-world
🔐 Step 4: Configure SSH Access from Jenkins to DockerHost
On Jenkins instance:
bash
Copy
Edit
ssh-keygen -t rsa -b 4096 -C "jenkins@dockerhost"
# Save at default path: /home/admin/.ssh/id_rsa
Copy Public Key to DockerHost
bash
Copy
Edit
ssh-copy-id -i ~/.ssh/id_rsa.pub admin@<DOCKER_HOST_IP>
Test SSH:

bash
Copy
Edit
ssh admin@<DOCKER_HOST_IP>
🛠️ Step 5: Prepare GitHub Repo
Create a repo (e.g., nginx-web-deploy) with:

plaintext
Copy
Edit
index.html
Dockerfile
Example index.html
html
Copy
Edit
<!DOCTYPE html>
<html>
  <head><title>My Website</title></head>
  <body><h1>Deployed with Jenkins CI/CD!</h1></body>
</html>
Example Dockerfile
Dockerfile
Copy
Edit
FROM nginx:alpine
COPY index.html /usr/share/nginx/html/index.html
Push these to GitHub.

🔄 Step 6: Configure Jenkins Job
Install Plugins
GitHub Integration

Docker Pipeline

SSH Pipeline Steps

Create New Pipeline Job
Jenkins → New Item → Pipeline → OK

Add GitHub Repo
Configure GitHub repo URL and credentials (if private).

Add Pipeline Script
groovy
Copy
Edit
pipeline {
  agent any
  stages {
    stage('Clone Repo') {
      steps {
        git 'https://github.com/<your-username>/nginx-web-deploy.git'
      }
    }

    stage('Build Docker Image') {
      steps {
        sh '''
        docker build -t mynginx-web .
        docker save mynginx-web > mynginx-web.tar
        '''
      }
    }

    stage('Deploy to Remote Docker Host') {
      steps {
        sh '''
        scp -o StrictHostKeyChecking=no mynginx-web.tar admin@<DOCKER_HOST_IP>:/tmp/
        ssh -o StrictHostKeyChecking=no admin@<DOCKER_HOST_IP> << EOF
          docker load < /tmp/mynginx-web.tar
          docker stop web || true && docker rm web || true
          docker run -d --name web -p 80:80 mynginx-web
        EOF
        '''
      }
    }
  }
}
🔁 Step 7: Automate via Webhooks (Optional)
In your GitHub repo → Settings → Webhooks

Add a webhook: http://<JENKINS_PUBLIC_IP>:8080/github-webhook/

Choose Push Events

In Jenkins job → Configure → Build Triggers → Enable GitHub hook trigger

✅ Final Result
Commit to GitHub triggers Jenkins

Jenkins builds Docker image and transfers to deploy server

Docker runs NGINX with your static site

Access via: http://<DOCKER_HOST_PUBLIC_IP>

📌 Security Tips
Limit SSH access via security groups

Use SSH keys only (disable password auth)

Store secrets via Jenkins credentials

Consider Docker image cleanup cronjobs

Would you like a complete Terraform script or Ansible playbook to automate all of this?







You said:
same for this question Create a CI/CD pipeline using Jenkins, Docker, and GitHub to automate the web by using the httpd image.


ChatGPT said:
Here's a step-by-step guide to build a CI/CD pipeline using Jenkins, Docker, and GitHub, using the Apache HTTPD image (instead of NGINX), with 2 Debian-based AWS EC2 instances.

🔧 Architecture
Instance 1 (Jenkins Controller): Runs Jenkins

Instance 2 (Deploy Host): Runs Docker + serves website with httpd (Apache)

GitHub Repo: Contains static website files (index.html) and a Dockerfile using httpd

✅ Step-by-Step Instructions
1️⃣ Launch Two Debian EC2 Instances
OS: Debian 12

Instance 1: Jenkins (jenkins-server)

Instance 2: Docker + Apache HTTPD (deploy-server)

Open Security Group Ports:

Jenkins (8080) on instance 1

HTTP (80) on instance 2

SSH (22) on both

2️⃣ Install Jenkins on Instance 1
bash
Copy
Edit
sudo apt update && sudo apt install -y openjdk-17-jdk curl gnupg2 git
curl -fsSL https://pkg.jenkins.io/debian-stable/jenkins.io-2023.key | sudo tee \
  /usr/share/keyrings/jenkins-keyring.asc > /dev/null

echo deb [signed-by=/usr/share/keyrings/jenkins-keyring.asc] \
  https://pkg.jenkins.io/debian-stable binary/ | sudo tee \
  /etc/apt/sources.list.d/jenkins.list > /dev/null

sudo apt update
sudo apt install -y jenkins
sudo systemctl enable jenkins
sudo systemctl start jenkins
Visit http://<JENKINS_PUBLIC_IP>:8080 and unlock with:

bash
Copy
Edit
sudo cat /var/lib/jenkins/secrets/initialAdminPassword
3️⃣ Set Up Docker on Deploy Server (Instance 2)
bash
Copy
Edit
sudo apt update && sudo apt install -y docker.io apache2
sudo systemctl enable docker && sudo systemctl start docker
Test Docker:

bash
Copy
Edit
sudo docker run hello-world
4️⃣ Allow Jenkins to SSH into Deploy Server
On Jenkins instance:

bash
Copy
Edit
ssh-keygen -t rsa -b 4096 -C "jenkins@deploy"
ssh-copy-id -i ~/.ssh/id_rsa.pub admin@<DEPLOY_SERVER_IP>
ssh admin@<DEPLOY_SERVER_IP>  # test login
5️⃣ GitHub Repo Setup
Create a GitHub repo (e.g., httpd-web-deploy) with:

index.html
html
Copy
Edit
<!DOCTYPE html>
<html>
  <head><title>Apache HTTPD Web</title></head>
  <body><h1>Deployed via Jenkins CI/CD and Docker!</h1></body>
</html>
Dockerfile
Dockerfile
Copy
Edit
FROM httpd:alpine
COPY index.html /usr/local/apache2/htdocs/index.html
Push this repo to GitHub.

6️⃣ Jenkins Job Configuration
Install Plugins:
GitHub

Docker Pipeline

SSH Steps

Create a New Pipeline Job:
Go to Jenkins → New Item → Pipeline

Name: Apache-CI-CD-Pipeline

Scroll to Pipeline Section and paste:

groovy
Copy
Edit
pipeline {
  agent any

  stages {
    stage('Clone Repo') {
      steps {
        git 'https://github.com/<your-username>/httpd-web-deploy.git'
      }
    }

    stage('Build Docker Image') {
      steps {
        sh '''
        docker build -t apache-web .
        docker save apache-web > apache-web.tar
        '''
      }
    }

    stage('Deploy to Remote Docker Host') {
      steps {
        sh '''
        scp -o StrictHostKeyChecking=no apache-web.tar admin@<DEPLOY_SERVER_IP>:/tmp/
        ssh -o StrictHostKeyChecking=no admin@<DEPLOY_SERVER_IP> << EOF
          docker load < /tmp/apache-web.tar
          docker stop apache || true && docker rm apache || true
          docker run -d --name apache -p 80:80 apache-web
        EOF
        '''
      }
    }
  }
}
Replace <your-username> and <DEPLOY_SERVER_IP> with your actual values.

7️⃣ Add GitHub Webhook (Optional)
In your GitHub repo:

Settings → Webhooks → Add webhook

Payload URL: http://<JENKINS_PUBLIC_IP>:8080/github-webhook/

Content type: application/json

Trigger: Just the push event

In Jenkins Job → Configure → Build Triggers → check GitHub hook trigger for GITScm polling

✅ Result
Every GitHub push will:

Trigger Jenkins build

Build Docker image with httpd

Send and run it on remote EC2 deploy server

Serve site at http://<DEPLOY_SERVER_PUBLIC_IP>

🛡️ Security and Production Tips
Harden SSH access (key-based only)

Use IAM roles and Jenkins credentials securely

Set up Jenkins backup and security

Auto-clean Docker containers/images over time

Would you like to see a fully automated setup with Terraform or Ansible to provision these servers and deploy the pipeline?







You said:
Create AWS VPC with step by step instructions


ChatGPT said:
Creating an AWS VPC (Virtual Private Cloud) manually is a fundamental task when setting up infrastructure in the cloud. Here's a step-by-step guide using the AWS Management Console (GUI). If you'd prefer a Terraform or AWS CLI version afterward, let me know.

🧰 Prerequisites
An active AWS account

IAM user with permissions for VPC, subnets, route tables, internet gateway, etc.

🌐 Step-by-Step: Create a VPC in AWS Console
1️⃣ Log In to AWS Console
Navigate to: https://console.aws.amazon.com/vpc/

2️⃣ Create VPC
Go to VPC Dashboard

Click “Create VPC”

Choose VPC Only (or “VPC and more” if you want subnets and internet gateway together)

Fill in:

Name tag: My-VPC

IPv4 CIDR block: 10.0.0.0/16 (adjust as needed)

Tenancy: Default

Click Create VPC

✅ Your VPC is now created.

3️⃣ Create Subnets
You typically create at least:

1 Public Subnet (with internet access)

1 Private Subnet (for backend services)

Create Public Subnet:
Go to Subnets → Create Subnet

Select your VPC (My-VPC)

Subnet name: Public-Subnet-1

Availability Zone: us-east-1a (or choose yours)

IPv4 CIDR block: 10.0.1.0/24

Click Create Subnet

Create Private Subnet:
Repeat steps above with:

Subnet name: Private-Subnet-1

CIDR block: 10.0.2.0/24

4️⃣ Create and Attach an Internet Gateway
Go to Internet Gateways → Create Internet Gateway

Name: My-IGW

Click Create Internet Gateway

Select the new IGW → Actions → Attach to VPC

Choose My-VPC → Attach

5️⃣ Create Route Table
Go to Route Tables → Create route table

Name: Public-RT

Select My-VPC

Click Create route table

Add Internet Route:
Select Public-RT

Routes → Edit routes → Add route:

Destination: 0.0.0.0/0

Target: your Internet Gateway (igw-xxxx)

Save routes

Associate Public Subnet:
Go to Subnet Associations

Edit subnet associations

Select Public-Subnet-1 → Save

6️⃣ Modify Public Subnet to Auto-Assign Public IPs
Go to Subnets

Click Public-Subnet-1

Edit subnet settings

Enable Auto-assign public IPv4 address

Save

7️⃣ (Optional) NAT Gateway for Private Subnet Access
To allow private subnet to access the internet (e.g., for updates):

Go to Elastic IPs → Allocate Elastic IP

Allocate

Go to NAT Gateway → Create NAT Gateway

Subnet: Public-Subnet-1

Elastic IP: choose the one you just created

Create

Go to Route Tables → Create route table → name it Private-RT

Edit routes → Add route:

Destination: 0.0.0.0/0

Target: NAT Gateway

Associate Private-Subnet-1 with Private-RT

✅ Summary of Resources Created
Resource	CIDR / Purpose	Notes
VPC	10.0.0.0/16	Your private network
Public Subnet	10.0.1.0/24	For EC2 with internet
Private Subnet	10.0.2.0/24	For internal services
Internet Gateway	n/a	Enables internet access
Route Table	Public	Internet access via IGW
NAT Gateway	(optional)	Private subnet internet access

Would you like a Terraform version of this setup for automated provisioning?







You said:
Create a new VPC by the name ditiss-lab with public & private subnet. Assign network address as 172.20.0.0/16. Assign 172.20.5.0/24 to the public and 172.20.10.0/24 to the private subnet. Create one instance and connect to a private subnet. Create one instance and connect to the public subnet. Install httpd on the private instance and check with curl from the public connected instance. with step by step instruction


ChatGPT said:
Here’s a step-by-step guide to create a custom AWS VPC named ditiss-lab, set up public and private subnets, deploy EC2 instances in each, install httpd (Apache) on the private instance, and test access from the public instance using curl.

🎯 GOAL
Component	Value
VPC Name	ditiss-lab
CIDR Block	172.20.0.0/16
Public Subnet	172.20.5.0/24
Private Subnet	172.20.10.0/24
Public EC2	Amazon Linux 2, with public IP
Private EC2	Amazon Linux 2, no public IP
Apache Access	Check from public to private with curl

🧱 STEP-BY-STEP SETUP (Console-Based)
1️⃣ Create VPC
Go to VPC Dashboard → Create VPC

Choose VPC Only

Name: ditiss-lab

IPv4 CIDR block: 172.20.0.0/16

Leave rest as default

Click Create VPC

2️⃣ Create Subnets
Public Subnet
Go to Subnets → Create subnet

Name: public-subnet-1

VPC: ditiss-lab

AZ: us-east-1a (or your preferred)

CIDR block: 172.20.5.0/24

Private Subnet
Create another:

Name: private-subnet-1

VPC: ditiss-lab

AZ: us-east-1a

CIDR block: 172.20.10.0/24

3️⃣ Create and Attach Internet Gateway
Go to Internet Gateways → Create

Name: ditiss-igw

Create and attach to VPC: ditiss-lab

4️⃣ Create and Configure Route Tables
Public Route Table:
Go to Route Tables → Create

Name: public-rt

Select VPC: ditiss-lab

Edit routes:

Add route: 0.0.0.0/0 → Target: Internet Gateway

Associate with: public-subnet-1

Private Route Table:
Go to Route Tables → Create

Name: private-rt

Select VPC: ditiss-lab

No internet access needed

Associate with: private-subnet-1

5️⃣ Enable Auto-assign Public IP for Public Subnet
Go to Subnets

Select public-subnet-1

Actions → Modify auto-assign IP

Check Enable auto-assign public IPv4

6️⃣ Create EC2 Key Pair
Go to EC2 → Key Pairs → Create

Name: ditiss-key

Download .pem file

7️⃣ Launch EC2 Instances
Public EC2:
AMI: Amazon Linux 2

Subnet: public-subnet-1

Auto-assign Public IP: Enabled

Security Group:

Allow SSH (22) and HTTP (80) from your IP

Key Pair: ditiss-key

Private EC2:
AMI: Amazon Linux 2

Subnet: private-subnet-1

Auto-assign Public IP: Disabled

Security Group:

Allow SSH from public EC2’s security group

Allow HTTP (80) from public EC2’s security group

Key Pair: ditiss-key

8️⃣ SSH into Public EC2
bash
Copy
Edit
ssh -i ditiss-key.pem ec2-user@<PUBLIC_INSTANCE_IP>
9️⃣ SSH into Private EC2 via Public EC2 (Bastion)
From public EC2:

bash
Copy
Edit
ssh -i ditiss-key.pem ec2-user@<PRIVATE_INSTANCE_IP>
🔧 10️⃣ Install Apache on Private EC2
bash
Copy
Edit
sudo yum update -y
sudo yum install httpd -y
sudo systemctl start httpd
sudo systemctl enable httpd
echo "<h1>Hello from Private Instance</h1>" | sudo tee /var/www/html/index.html
✅ 11️⃣ Test HTTP from Public EC2
From public EC2:

bash
Copy
Edit
curl http://<PRIVATE_INSTANCE_IP>
You should see:

css
Copy
Edit
<h1>Hello from Private Instance</h1>
🧪 BONUS: Verify Setup
Private EC2 should not be accessible directly from the internet

Public EC2 should be able to access private EC2 via HTTP

Apache should serve content on port 80 inside private subnet

🎁 Want It Automated?
I can generate a Terraform script, CloudFormation template, or Ansible playbook to provision this entire setup. Want that?







You said:
Create a new docker image using NGINX. Push it to docker hub. Create a service on Dockerswarm. Keep initial replicas as 1 and then increase the replicas to 10. Reduce the replicas to 2. Create a new image by modifying “index.html”. Push to docker hub and then update the service for a new image.


ChatGPT said:
Here’s a complete step-by-step guide to:

Create a custom Docker image using NGINX

Push it to Docker Hub

Deploy it as a service on Docker Swarm

Scale the service replicas

Update the image and redeploy

🧰 Prerequisites
Docker installed

Docker Hub account

Docker Swarm initialized (docker swarm init)

Logged in to Docker Hub (docker login)

🔧 STEP 1: Create a Docker Image with NGINX
bash
Copy
Edit
mkdir nginx-app
cd nginx-app
index.html
html
Copy
Edit
<!DOCTYPE html>
<html>
  <head><title>NGINX on Swarm</title></head>
  <body><h1>Version 1: Hello from Docker Swarm!</h1></body>
</html>
Dockerfile
Dockerfile
Copy
Edit
FROM nginx:alpine
COPY index.html /usr/share/nginx/html/index.html
🐳 STEP 2: Build and Push Image to Docker Hub
Replace yourusername with your Docker Hub username:

bash
Copy
Edit
docker build -t yourusername/nginx-swarm:v1 .
docker push yourusername/nginx-swarm:v1
⚙️ STEP 3: Create Docker Swarm Service
bash
Copy
Edit
docker service create \
  --name nginx-service \
  --replicas 1 \
  -p 80:80 \
  yourusername/nginx-swarm:v1
✅ Check:

bash
Copy
Edit
docker service ls
docker service ps nginx-service
🔁 STEP 4: Scale Service Replicas
Increase to 10 replicas:
bash
Copy
Edit
docker service scale nginx-service=10
Decrease to 2 replicas:
bash
Copy
Edit
docker service scale nginx-service=2
✏️ STEP 5: Update Image with Modified Content
Edit your index.html:

html
Copy
Edit
<h1>Version 2: Updated Content!</h1>
Rebuild and push with a new tag:

bash
Copy
Edit
docker build -t yourusername/nginx-swarm:v2 .
docker push yourusername/nginx-swarm:v2
🔄 STEP 6: Update Swarm Service to Use New Image
bash
Copy
Edit
docker service update \
  --image yourusername/nginx-swarm:v2 \
  nginx-service
✅ Check:

bash
Copy
Edit
docker service ps nginx-service
Visit your Swarm node’s IP in the browser or use curl:

bash
Copy
Edit
curl http://localhost
It should show:

yaml
Copy
Edit
Version 2: Updated Content!
✅ Summary
Task	Command
Build Image	docker build -t your/image .
Push to Docker Hub	docker push your/image
Create Service	docker service create --replicas 1 ...
Scale Service	docker service scale service=10
Update Image	docker service update --image new-image service

Would you like a script to automate the entire flow from build to deployment and update?







You said:
Configure a Linux as a Master node using Kubernetes and make 2 to 3 worker node clusters.


ChatGPT said:
Here’s a step-by-step guide to configure a Kubernetes master node (control plane) on Linux and join 2–3 worker nodes to form a functional cluster.

This uses kubeadm, which is the most straightforward method for creating a Kubernetes cluster on bare-metal Linux systems (e.g., Ubuntu/Debian/CentOS).

⚙️ ENVIRONMENT OVERVIEW
Role	Nodes	OS
Master	master-node	Ubuntu/Debian
Workers	worker-node-1, worker-node-2, (optional worker-node-3)	Ubuntu/Debian

All nodes should have:

Unique hostnames

Internet access

Open ports for Kubernetes

🧰 PREREQUISITES
At least 2 vCPUs and 2 GB RAM per node

All nodes can communicate via private IP

SSH access with sudo privileges

🔄 STEP 1: COMMON SETUP FOR ALL NODES
Run these commands on all master and worker nodes:

1.1 Update and Install Dependencies
bash
Copy
Edit
sudo apt update && sudo apt upgrade -y
sudo apt install -y apt-transport-https ca-certificates curl gnupg lsb-release
1.2 Disable Swap
bash
Copy
Edit
sudo swapoff -a
sudo sed -i '/ swap / s/^/#/' /etc/fstab
1.3 Set Hostnames
bash
Copy
Edit
# On master
sudo hostnamectl set-hostname master-node

# On workers
sudo hostnamectl set-hostname worker-node-1
# Repeat for worker-node-2, etc.
1.4 Add Kubernetes APT Repo and Install Tools
bash
Copy
Edit
curl -fsSL https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add -
echo "deb https://apt.kubernetes.io/ kubernetes-xenial main" | \
  sudo tee /etc/apt/sources.list.d/kubernetes.list

sudo apt update
sudo apt install -y kubelet kubeadm kubectl
sudo apt-mark hold kubelet kubeadm kubectl
📦 STEP 2: INSTALL CONTAINER RUNTIME (containerd)
2.1 Install containerd
bash
Copy
Edit
sudo apt install -y containerd
2.2 Configure containerd
bash
Copy
Edit
sudo mkdir -p /etc/containerd
containerd config default | sudo tee /etc/containerd/config.toml
Set systemd as cgroup driver in /etc/containerd/config.toml:

ini
Copy
Edit
[plugins."io.containerd.grpc.v1.cri".containerd.runtimes.runc.options]
  SystemdCgroup = true
Restart:

bash
Copy
Edit
sudo systemctl restart containerd
sudo systemctl enable containerd
🌐 STEP 3: SETUP K8s MASTER NODE
Only on the master node:

3.1 Initialize Control Plane
bash
Copy
Edit
sudo kubeadm init --pod-network-cidr=192.168.0.0/16
This will output a kubeadm join command — copy and save it.

3.2 Configure kubectl for Admin User
bash
Copy
Edit
mkdir -p $HOME/.kube
sudo cp /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config
3.3 Install Network Plugin (e.g., Calico)
bash
Copy
Edit
kubectl apply -f https://raw.githubusercontent.com/projectcalico/calico/v3.27.0/manifests/calico.yaml
Wait a few seconds and check:

bash
Copy
Edit
kubectl get nodes
🔗 STEP 4: JOIN WORKER NODES
On each worker node, run the kubeadm join command copied earlier:

bash
Copy
Edit
sudo kubeadm join <MASTER_IP>:6443 --token <token> --discovery-token-ca-cert-hash sha256:<hash>
✅ STEP 5: VERIFY CLUSTER
On the master node:

bash
Copy
Edit
kubectl get nodes
You should see something like:

bash
Copy
Edit
NAME            STATUS   ROLES           AGE   VERSION
master-node     Ready    control-plane   10m   v1.28.0
worker-node-1   Ready    <none>          2m    v1.28.0
worker-node-2   Ready    <none>          2m    v1.28.0
🛠 OPTIONAL: Taints and Labels
To allow scheduling on master (not recommended for prod):

bash
Copy
Edit
kubectl taint nodes master-node node-role.kubernetes.io/control-plane-
To label worker nodes:

bash
Copy
Edit
kubectl label node worker-node-1 node-role.kubernetes.io/worker=worker
📋 Summary of Commands
Purpose	Command
Disable swap	swapoff -a
Install Kubernetes	apt install -y kubelet kubeadm kubectl
Init cluster	kubeadm init
Join workers	kubeadm join <...>
Install Calico	kubectl apply -f calico.yaml

Would you like a shell script to automate this for all nodes?
